{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pytorch-ignite\n",
    "# from google.colab import drive\n",
    "# drive.flush_and_unmount()\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Sq8uv4c9S3-A"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from src.datasets.datasets import MovielensDataset\n",
    "from src.models.mlstm import mLSTM\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import datetime\n",
    "\n",
    "# datetime object containing current date and time\n",
    "script_start_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%Hh%Mm%S\")\n",
    "\n",
    "find_lr = False\n",
    "\n",
    "ablation_types = ['movie_network', 'genre', 'user_history', 'user_ratings', 'user_info', 'title']\n",
    "ablation = None\n",
    "rnn_type = 'lstm'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51KGxlUHT-i3"
   },
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "z7LqqL_8rrHJ"
   },
   "outputs": [],
   "source": [
    "class ScaledEmbedding(nn.Embedding):\n",
    "    \"\"\"\n",
    "    Embedding layer that initialises its values\n",
    "    to using a normal variable scaled by the inverse\n",
    "    of the embedding dimension.\n",
    "    \"\"\"\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"\n",
    "        Initialize parameters.\n",
    "        \"\"\"\n",
    "\n",
    "        self.weight.data.normal_(0, 1.0 / self.embedding_dim)\n",
    "        if self.padding_idx is not None:\n",
    "            self.weight.data[self.padding_idx].fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "BDgV-CJ6UGiF"
   },
   "outputs": [],
   "source": [
    "PADDING_IDX = 0\n",
    "class mLSTMNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Module representing users through running a recurrent neural network\n",
    "    over the sequence, using the hidden state at each timestep as the\n",
    "    sequence representation, a'la [2]_\n",
    "    During training, representations for all timesteps of the sequence are\n",
    "    computed in one go. Loss functions using the outputs will therefore\n",
    "    be aggregating both across the minibatch and across time in the sequence.\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_items: int\n",
    "        Number of items to be represented.\n",
    "    embedding_dim: int, optional\n",
    "        Embedding dimension of the embedding layer, and the number of hidden\n",
    "        units in the LSTM layer.\n",
    "    item_embedding_layer: an embedding layer, optional\n",
    "        If supplied, will be used as the item embedding layer\n",
    "        of the network.\n",
    "    References\n",
    "    ----------\n",
    "    .. [2] Hidasi, Balazs, et al. \"Session-based recommendations with\n",
    "       recurrent neural networks.\" arXiv preprint arXiv:1511.06939 (2015).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_items, embedding_dim=32, sparse=False, rnn_type='mlstm', ablation=None):\n",
    "\n",
    "        super(mLSTMNet, self).__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.item_embeddings = ScaledEmbedding(num_items, embedding_dim,\n",
    "                                                padding_idx=PADDING_IDX,\n",
    "                                                sparse=sparse)\n",
    "\n",
    "        self.ablation = ablation\n",
    "        self.rnn_type = rnn_type\n",
    "        title_dim = 0\n",
    "        ubranch_dim = 0\n",
    "        user_history_dim = 0\n",
    "        movie_dim = 0\n",
    "        \n",
    "            \n",
    "\n",
    "            \n",
    "            \n",
    "        if ablation != 'movie_network':\n",
    "            movie_dim = movie_dim + embedding_dim\n",
    "        if ablation != 'genre':\n",
    "            movie_dim = movie_dim + 18\n",
    "        \n",
    "        if movie_dim > 0:\n",
    "            self.mbranch = nn.Sequential(\n",
    "                nn.Linear(movie_dim, movie_dim),\n",
    "                nn.ReLU(movie_dim),\n",
    "                nn.Dropout(0.5),\n",
    "                \n",
    "                nn.Linear(movie_dim, movie_dim),\n",
    "                nn.ReLU(movie_dim),\n",
    "                nn.Dropout(0.5),\n",
    "                \n",
    "                nn.Linear(movie_dim, movie_dim),\n",
    "                nn.ReLU(movie_dim),\n",
    "            )\n",
    "        \n",
    "        if ablation != 'title':\n",
    "            title_input_dim = 384\n",
    "            title_dim = title_input_dim // 2\n",
    "            self.tbranch = nn.Sequential(                \n",
    "                nn.Linear(title_input_dim, title_dim),\n",
    "                nn.ReLU(title_dim),\n",
    "            )\n",
    "\n",
    "            \n",
    "        if ablation != 'user_info':\n",
    "            ubranch_dim = 21 + 1 + 2 \n",
    "            self.ubranch = nn.Sequential(\n",
    "                nn.Linear(ubranch_dim, ubranch_dim),\n",
    "                nn.ReLU(ubranch_dim),\n",
    "                nn.Dropout(0.5),\n",
    "                \n",
    "                nn.Linear(ubranch_dim, ubranch_dim),\n",
    "                nn.ReLU(ubranch_dim),\n",
    "                nn.Dropout(0.5),\n",
    "                \n",
    "                nn.Linear(ubranch_dim, ubranch_dim),\n",
    "                nn.ReLU(ubranch_dim),\n",
    "            )\n",
    "            \n",
    "        \n",
    "        if ablation != 'user_history':\n",
    "            \n",
    "            if ablation != 'user_ratings':\n",
    "                user_history_dim = embedding_dim+1\n",
    "            else:\n",
    "                user_history_dim = embedding_dim\n",
    "                \n",
    "            h_init = torch.zeros(user_history_dim)\n",
    "            h_init.normal_(0, 1.0 / self.embedding_dim)\n",
    "            self.h_init = nn.Parameter(h_init, requires_grad=True)\n",
    "            if self.rnn_type == 'mlstm':\n",
    "\n",
    "                self.mlstm = mLSTM(input_size=user_history_dim,\n",
    "                                   hidden_size=user_history_dim)\n",
    "            else:\n",
    "                self.lstm = nn.LSTM(\n",
    "                    input_size=user_history_dim,\n",
    "                    hidden_size=user_history_dim,\n",
    "                    batch_first=True\n",
    "                )\n",
    "\n",
    "            \n",
    "        self.user_history_dim = user_history_dim\n",
    "            \n",
    "\n",
    "        ## THIS IS IRRELEVANT - just for making an ratings output\n",
    "        ################################################################################################\n",
    "        concat_dim = user_history_dim+ubranch_dim+movie_dim+title_dim\n",
    "        self.rating  = nn.Sequential(\n",
    "            # nn.Dropout(0.5),\n",
    "            # nn.Linear(user_history_dim+ubranch_dim+movie_dim+title_dim, embedding_dim),\n",
    "            # nn.ReLU(embedding_dim),\n",
    "            # nn.Dropout(0.5),\n",
    "            # nn.Linear(embedding_dim, 1)\n",
    "            \n",
    "            \n",
    "            \n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            nn.Linear(concat_dim, concat_dim // 2),\n",
    "            nn.ReLU(concat_dim // 2),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            nn.Linear(concat_dim // 2, concat_dim // 2),\n",
    "            nn.ReLU(concat_dim // 2),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            \n",
    "            nn.Linear(concat_dim // 2, 1)\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def ablate(self, ablation_eval=()):\n",
    "        if 'movie' in ablation_eval:\n",
    "            self.mbranch.requires_grad_(False)\n",
    "            self.mbranch.apply(self._ablate_weights)\n",
    "        \n",
    "        if 'title' in ablation_eval:\n",
    "            self.tbranch.requires_grad_(False)\n",
    "            self.tbranch.apply(self._ablate_weights)\n",
    "        \n",
    "        if 'user_info' in ablation_eval:\n",
    "            self.ubranch.requires_grad_(False)\n",
    "            self.ubranch.apply(self._ablate_weights)\n",
    "            \n",
    "        if 'user_history' in ablation_eval:\n",
    "            if self.rnn_type == 'mlstm':\n",
    "                self.mlstm.requires_grad_(False)\n",
    "                self.mlstm.lstm_cell.weight_ih.data.fill_(0.0)\n",
    "                self.mlstm.lstm_cell.weight_hh.data.fill_(0.0)\n",
    "                \n",
    "                self.mlstm.lstm_cell.bias_ih.data[self.user_history_dim:2*self.user_history_dim].fill_(-float('inf'))\n",
    "                self.mlstm.lstm_cell.bias_hh.data[self.user_history_dim:2*self.user_history_dim].fill_(-float('inf'))\n",
    "                \n",
    "\n",
    "            elif self.rnn_type =='lstm':\n",
    "                self.lstm.requires_grad_(False)\n",
    "                self.lstm.weight_ih_l0.data.fill_(0.0)\n",
    "                self.lstm.weight_hh_l0.data.fill_(0.0)\n",
    "                \n",
    "                self.lstm.bias_ih_l0.data[self.user_history_dim:2*self.user_history_dim].fill_(-float('inf'))\n",
    "                self.lstm.bias_hh_l0.data[self.user_history_dim:2*self.user_history_dim].fill_(-float('inf'))\n",
    "        \n",
    "    def _ablate_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            m.weight.data.fill_(0.0)\n",
    "            \n",
    "    def forward(self, item_sequences=None, ratings_sequences=None, lengths=None, movies=None, user_info=None, mgenre=None, mtitle=None):\n",
    "        \"\"\"\n",
    "        Compute user representation from a given sequence.\n",
    "        Returns\n",
    "        -------\n",
    "        tuple (all_representations, final_representation)\n",
    "            The first element contains all representations from step\n",
    "            -1 (no items seen) to t - 1 (all but the last items seen).\n",
    "            The second element contains the final representation\n",
    "            at step t (all items seen). This final state can be used\n",
    "            for prediction or evaluation.\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        if self.ablation != 'movie_network':\n",
    "            movie_embedding = (self.item_embeddings(torch.unsqueeze(movies, 0)).squeeze(0))\n",
    "            if self.ablation != 'genre':\n",
    "                mov_emb = torch.cat((movie_embedding, mgenre), dim=1)\n",
    "            else:            \n",
    "                mov_emb = movie_embedding\n",
    "\n",
    "\n",
    "        elif self.ablation != 'genre':\n",
    "            mov_emb = mgenre\n",
    "\n",
    "            \n",
    "        mov_emb = self.mbranch(mov_emb)\n",
    "        \n",
    "        # if 'movie' in ablation_eval:\n",
    "        #     mov_emb = torch.zeros_like(mov_emb)\n",
    "        \n",
    "\n",
    "        if self.ablation != 'title':\n",
    "            tit_emb = self.tbranch(mtitle)\n",
    "            \n",
    "            # if 'title' in ablation_eval:\n",
    "            #     tit_emb = torch.zeros_like(tit_emb)\n",
    "\n",
    "                \n",
    "            \n",
    "\n",
    "        if self.ablation != 'user_info':\n",
    "            user_emb = self.ubranch(user_info)\n",
    "            # if 'user_info' in ablation_eval:\n",
    "            #     user_emb = torch.zeros_like(user_emb)\n",
    "                \n",
    "        if self.ablation != 'user_history':\n",
    "            # Make the embedding dimension the channel dimension\n",
    "            sequence_embeddings = self.item_embeddings(item_sequences)\n",
    "            batch_size, seq_len, _ = sequence_embeddings.size()\n",
    "            embedding_dim = self.h_init.size()[0]\n",
    "            seq_start = self.h_init.expand(batch_size, embedding_dim)\n",
    "            # pad from left with initial state\n",
    "            if self.ablation != 'user_ratings':\n",
    "                ratings_embedding = (ratings_sequences.unsqueeze(2) -  3)/(1.155*self.embedding_dim)\n",
    "\n",
    "                X= torch.cat([sequence_embeddings, ratings_embedding], dim=2)\n",
    "            else:\n",
    "                X=sequence_embeddings\n",
    "\n",
    "            if self.rnn_type == 'mlstm':\n",
    "                X = self.mlstm(X, (seq_start, seq_start))\n",
    "                user_representations = X.permute(0, 2, 1)[:, :, -1]\n",
    "            else:\n",
    "                seq_start = seq_start.unsqueeze(0)\n",
    "\n",
    "                X = torch.nn.utils.rnn.pack_padded_sequence(X, lengths,enforce_sorted=False, batch_first=True)                \n",
    "                X, _ = self.lstm(X, (seq_start, seq_start))\n",
    "                X, _ = torch.nn.utils.rnn.pad_packed_sequence(X, batch_first=True)\n",
    "\n",
    "                # X = X.contiguous()\n",
    "                user_representations =  X.permute(0, 2, 1)[:, :, -1] #.view(-1, X.shape[2])\n",
    "                \n",
    "            # if 'user_history' in ablation_eval:\n",
    "            #     user_representations = torch.zeros_like(user_representations)\n",
    "                \n",
    "\n",
    "\n",
    "        ## user_representations[:, :, -1] is your user history representation (lstm output - )\n",
    "        ## make modifications after this point\n",
    "        ###################################################################################\n",
    "\n",
    "        # print(movie_embedding.shape, user_representations.shape)\n",
    "        \n",
    "        if self.ablation == 'user_info':\n",
    "            x = torch.cat([user_representations, mov_emb, tit_emb], dim=1)\n",
    "        elif self.ablation == 'user_history':\n",
    "            x = torch.cat([user_emb, mov_emb, tit_emb], dim=1)\n",
    "        elif self.ablation == 'title':\n",
    "            # for k in [user_representations, user_emb, mov_emb, genr_emb]:\n",
    "            #     print(k.size())\n",
    "            x = torch.cat([user_representations, user_emb, mov_emb], dim=1)\n",
    "        else:\n",
    "            x = torch.cat([user_representations, user_emb, mov_emb, tit_emb], dim=1)\n",
    "        return self.rating(x).squeeze(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8-Y34xTWR7Q"
   },
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "L2iWPo4al04d"
   },
   "outputs": [],
   "source": [
    "common_space = {\n",
    "    'batch_size': 512,\n",
    "    'learn_rate': 5e-04,\n",
    "    'l2': 1e-04,\n",
    "    'n_iter': 40,\n",
    "    'embedding_dim': 110,\n",
    "}\n",
    "\n",
    "space = common_space\n",
    "\n",
    "batch_size = int(space['batch_size'])\n",
    "learn_rate = space['learn_rate']\n",
    "n_iter = int(space['n_iter'])\n",
    "embedding_dim = int(space['embedding_dim'])\n",
    "l2 = space['l2']\n",
    "\n",
    "early_stop_crit = 5\n",
    "min_epochs = n_iter*0.3+early_stop_crit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model/lstm_ablation-title/NoneAblation/ml-1m-split/2022-04-14_02h06m31\n"
     ]
    }
   ],
   "source": [
    "save_dir = os.path.join('data','ml-1m-split')\n",
    "results_folder = os.path.join('results',f'{rnn_type}_ablation-title',f'{ablation}Ablation', 'ml-1m-split', script_start_time)\n",
    "model_folder = os.path.join('model',f'{rnn_type}_ablation-title',f'{ablation}Ablation', 'ml-1m-split', script_start_time)\n",
    "\n",
    "\n",
    "\n",
    "# save_dir = os.path.join('/content/drive/MyDrive/ECSE552proj',save_dir)\n",
    "# results_folder = os.path.join('/content/drive/MyDrive/ECSE552proj',results_folder)\n",
    "# model_folder = os.path.join('/content/drive/MyDrive/ECSE552proj',model_folder)\n",
    "\n",
    "\n",
    "\n",
    "if not os.path.isdir(model_folder):\n",
    "    os.makedirs(model_folder)\n",
    "    print(model_folder)\n",
    "\n",
    "if not os.path.isdir(results_folder):\n",
    "    os.makedirs(results_folder)\n",
    "\n",
    "load_bools = (ablation != 'user_history', # user_history\n",
    "              ablation != 'user_info', # user_info\n",
    "              ablation != 'movie_info', # movie_info\n",
    "              rnn_type == 'lstm', # lengths\n",
    "              'post' if rnn_type == 'lstm' else 'pre', #pad\n",
    "              # None,\n",
    "              100 if rnn_type == 'mlstm' else 0, # max_history_length\n",
    "             ) \n",
    "train = MovielensDataset(os.path.join(save_dir,'users_train_dfs.h5'), os.path.join(save_dir,'movies_dfs.h5'), *load_bools)\n",
    "val = MovielensDataset(os.path.join(save_dir,'users_val_dfs.h5'), os.path.join(save_dir,'movies_dfs.h5'), *load_bools)\n",
    "test = MovielensDataset(os.path.join(save_dir,'users_test_dfs.h5'), os.path.join(save_dir,'movies_dfs.h5'), *load_bools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_collate(batch, key):\n",
    "    \n",
    "    padding_value = 0 if key == 'user_history_movie_id' else 3\n",
    "    if rnn_type == 'lstm':\n",
    "        return torch.nn.utils.rnn.pad_sequence([torch.as_tensor(x) for x in batch], batch_first=True, padding_value=padding_value)\n",
    "    else:\n",
    "        pad_to = max([len(x) for x in batch])\n",
    "        \n",
    "        return torch.vstack([F.pad(torch.as_tensor(x), (pad_to - len(x), 0), 'constant', constant=padding_value) for x in batch])\n",
    "    \n",
    "\n",
    "def collate(batch):\n",
    "    elem = batch[0]\n",
    "    elem_type = type(elem)\n",
    "    # print([key for key in elem])\n",
    "    d = {key: (pad_collate([d[key] for d in batch], key) if (key == 'user_history_movie_id' or key == 'user_history_rating')\n",
    "               else default_collate([d[key] for d in batch]))\n",
    "         for key in elem}\n",
    "    try:\n",
    "        return elem_type(d)\n",
    "    except TypeError:\n",
    "        # The mapping type may not support `__init__(iterable)`.\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader_train = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True, collate_fn=collate)\n",
    "# dataloader_test = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True, collate_fn=collate)\n",
    "# dataloader_val = torch.utils.data.DataLoader(val, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True, collate_fn=collate)\n",
    "\n",
    "dataloader_train = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "dataloader_test = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "dataloader_val = torch.utils.data.DataLoader(val, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for _, sample in enumerate(dataloader_train):\n",
    "#     one_samp = sample\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "_CsF24QOXAr0",
    "outputId": "54e79a43-9bd9-4d6b-cf5a-a102b1b22452"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is cuda!\n"
     ]
    }
   ],
   "source": [
    "print(\"device is {}!\".format(device))\n",
    "\n",
    "num_items = train.movies_genres_df.index.max()+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if find_lr:\n",
    "    from ignite.handlers import FastaiLRFinder\n",
    "    from ignite.engine.engine import Engine\n",
    "\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    def train_step(engine, sample):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()   \n",
    "\n",
    "        if ablation != 'user_history':\n",
    "            user_history_movie_id = sample['user_history_movie_id'].to(device)\n",
    "            if ablation != 'user_ratings':\n",
    "                user_history_rating = sample['user_history_rating'].to(torch.float32).to(device)\n",
    "            else:\n",
    "                user_history_rating = None\n",
    "        else:\n",
    "            user_history_movie_id = None\n",
    "            user_history_rating = None\n",
    "\n",
    "        movie_id = sample['movie_id'].to(device)\n",
    "        rating = sample['rating'].to(device)\n",
    "\n",
    "        if ablation != 'genre':\n",
    "            mgenre = sample['movie_info_genres'].to(device)\n",
    "        else:\n",
    "            mgenre = None\n",
    "\n",
    "        if ablation != 'user_info':\n",
    "            user_info = sample['user_info'].to(device)\n",
    "        else:\n",
    "            user_info = None\n",
    "\n",
    "        if ablation != 'title':\n",
    "            mtitle = sample['movie_info_title'].to(device)\n",
    "        else:\n",
    "            mtitle = None\n",
    "            \n",
    "        if rnn_type == 'lstm':\n",
    "            lengths = sample['user_history_len_cut'].cpu()\n",
    "        else:\n",
    "            lengths = None\n",
    "\n",
    "        # with torch.cuda.amp.autocast():\n",
    "        #     pred = model(user_history_movie_id, movie_id)\n",
    "\n",
    "        #     loss = criterion(pred, rating.to(torch.float16))\n",
    "\n",
    "        pred = model(item_sequences=user_history_movie_id,\n",
    "                      ratings_sequences=user_history_rating,\n",
    "                      lengths=lengths,\n",
    "                      movies=movie_id,\n",
    "                      user_info=user_info,\n",
    "                      mgenre=mgenre,\n",
    "                      mtitle=mtitle,\n",
    "                    ablation_eval=())\n",
    "\n",
    "        loss = torch.sqrt(criterion(pred, rating.to(torch.float32)))\n",
    "\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    trainer = Engine(train_step)\n",
    "\n",
    "\n",
    "    model = mLSTMNet(\n",
    "        num_items,\n",
    "        embedding_dim=embedding_dim,\n",
    "        rnn_type=rnn_type,\n",
    "        ablation=ablation)\n",
    "\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(\n",
    "                    model.parameters(),\n",
    "                    weight_decay=l2,\n",
    "                    lr=1e-06,\n",
    "                    betas=(0.9, 0.99),\n",
    "                    eps = 1e-05,\n",
    "                )\n",
    "\n",
    "\n",
    "\n",
    "    lr_finder = FastaiLRFinder()\n",
    "    to_save = {\"model\": model, \"optimizer\": optimizer}\n",
    "\n",
    "    with lr_finder.attach(trainer, to_save=to_save, diverge_th=5.0, end_lr=20) as trainer_with_lr_finder:\n",
    "        trainer_with_lr_finder.run(dataloader_train)\n",
    "\n",
    "    # Get lr_finder results\n",
    "    lr_finder.get_results()\n",
    "    plt.savefig(os.path.join(results_folder, 'lr_finder.png'))\n",
    "\n",
    "    # Plot lr_finder results (requires matplotlib)\n",
    "    lr_finder.plot()\n",
    "\n",
    "    # get lr_finder suggestion for lr\n",
    "    learn_rate = lr_finder.lr_suggestion()\n",
    "    print(learn_rate)\n",
    "    with open(os.path.join(results_folder, 'lr_finder.txt'), 'a') as the_file:\n",
    "        the_file.write(str(learn_rate))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "model = mLSTMNet(\n",
    "    num_items,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_type=rnn_type,\n",
    "    ablation=ablation)\n",
    "\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(\n",
    "                model.parameters(),\n",
    "                weight_decay=l2,\n",
    "                lr=learn_rate,\n",
    "                        betas=(0.9, 0.99),\n",
    "                    eps = 1e-05,\n",
    "            )\n",
    "\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=learn_rate, epochs=n_iter, steps_per_epoch=len(dataloader_train))\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_acc(dataloader, model):\n",
    "    loss_running = []\n",
    "    accu_running = []\n",
    "    mean_weights = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _, sample in enumerate(dataloader):\n",
    "            if ablation != 'user_history':\n",
    "                user_history_movie_id = sample['user_history_movie_id'].to(device)\n",
    "                if ablation != 'user_ratings':\n",
    "                    user_history_rating = sample['user_history_rating'].to(torch.float32).to(device)\n",
    "                else:\n",
    "                    user_history_rating = None\n",
    "            else:\n",
    "                user_history_movie_id = None\n",
    "                user_history_rating = None\n",
    "\n",
    "            movie_id = sample['movie_id'].to(device)\n",
    "            rating = sample['rating'].to(device)\n",
    "\n",
    "            if ablation != 'genre':\n",
    "                mgenre = sample['movie_info_genres'].to(device)\n",
    "            else:\n",
    "                mgenre = None\n",
    "\n",
    "            if ablation != 'user_info':\n",
    "                user_info = sample['user_info'].to(device)\n",
    "            else:\n",
    "                user_info = None\n",
    "\n",
    "            if ablation != 'title':\n",
    "                mtitle = sample['movie_info_title'].to(device)\n",
    "            else:\n",
    "                mtitle = None\n",
    "                \n",
    "            if rnn_type == 'lstm':\n",
    "                lengths = sample['user_history_len_cut'].cpu()\n",
    "            else:\n",
    "                lengths = None\n",
    "\n",
    "            # with torch.cuda.amp.autocast():\n",
    "            #     pred = model(user_history_movie_id, movie_id)\n",
    "\n",
    "            #     loss = criterion(pred, rating.to(torch.float16))\n",
    "\n",
    "            pred = model(item_sequences=user_history_movie_id,\n",
    "                         ratings_sequences=user_history_rating,\n",
    "                         lengths=lengths,\n",
    "                         movies=movie_id,\n",
    "                         user_info=user_info,\n",
    "                         mgenre=mgenre,\n",
    "                         mtitle=mtitle,)\n",
    "            pred_rounded = torch.clamp(pred,1,5).round()\n",
    "\n",
    "\n",
    "\n",
    "            loss = criterion(pred, rating.to(torch.float32))\n",
    "            accu = torch.sum(pred_rounded == rating.to(torch.int32)).to(torch.float32) / len(movie_id)\n",
    "            loss_running.append(loss.item())\n",
    "            accu_running.append(accu.item())\n",
    "\n",
    "            # we will weight average by batch size later\n",
    "            mean_weights.append(len(movie_id))\n",
    "\n",
    "    # print(len(loss_running), len(mean_weights))\n",
    "    return np.average(loss_running, weights=mean_weights), np.average(accu_running, weights=mean_weights)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start train...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "322b42264b5c407db5d4391f516228f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54168afb7685497fa98816594b6e75af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch:   0%|          | 0/797 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 train loss: 6.395897 validation loss: 1.285186 train accuracy: 0.171898 validation accuracy: 0.308625 next_lr: [2.8179502298273696e-05]\n",
      "epoch: 1 train loss: 1.425611 validation loss: 1.093821 train accuracy: 0.310949 validation accuracy: 0.343791 next_lr: [5.216047371103263e-05]\n",
      "epoch: 2 train loss: 1.247742 validation loss: 1.027553 train accuracy: 0.335451 validation accuracy: 0.363852 next_lr: [9.030831081506421e-05]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "# Initialize lists to store loss and accuracy values\n",
    "loss_history = []\n",
    "loss_history_val = []\n",
    "\n",
    "accu_history = []\n",
    "accu_history_val = []\n",
    "\n",
    "loss_history_running = []\n",
    "accu_history_running = []\n",
    "lr_history_running = []\n",
    "\n",
    "# Early Stopping\n",
    "best_loss_val = np.inf\n",
    "early_stop_count = 0\n",
    "\n",
    "\n",
    "# Train\n",
    "print(\"Start train...\")\n",
    "outer = tqdm(total=n_iter, desc='Epochs', position=0)\n",
    "inner = tqdm(total=len(dataloader_train), desc=f'Batch', position=1)\n",
    "for epoch in range(n_iter):\n",
    "\n",
    "    # check to see if validation loss has plateau'd\n",
    "    if early_stop_count >= early_stop_crit and epoch > min_epochs:\n",
    "        print(f'Validation loss plateaud; failed to improve after {early_stop_count} at {epoch}th epoch')\n",
    "        checkpoint = { \n",
    "            'epoch': epoch,\n",
    "            'model': model,\n",
    "            'optimizer': optimizer,\n",
    "            'scheduler': scheduler,\n",
    "            'scaler': scaler}\n",
    "        torch.save(checkpoint, os.path.join(model_folder, f'checkpt_earlystop_{epoch}.pth'))\n",
    "        break\n",
    "    \n",
    "    early_stop_count += 1\n",
    "    \n",
    "    #Train mode\n",
    "    model.train()\n",
    "    loss_running = []\n",
    "    accu_running = []\n",
    "    mean_weights = []\n",
    "    \n",
    "    inner.refresh()  #force print final state\n",
    "    inner.reset()  #reuse bar    \n",
    "    for _, sample in enumerate(dataloader_train):\n",
    "        lr_history_running.append(scheduler.get_last_lr())\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        if ablation != 'user_history':\n",
    "            user_history_movie_id = sample['user_history_movie_id'].to(device)\n",
    "            if ablation != 'user_ratings':\n",
    "                user_history_rating = sample['user_history_rating'].to(torch.float32).to(device)\n",
    "            else:\n",
    "                user_history_rating = None\n",
    "        else:\n",
    "            user_history_movie_id = None\n",
    "            user_history_rating = None\n",
    "\n",
    "        movie_id = sample['movie_id'].to(device)\n",
    "        rating = sample['rating'].to(device)\n",
    "\n",
    "        if ablation != 'genre':\n",
    "            mgenre = sample['movie_info_genres'].to(device)\n",
    "        else:\n",
    "            mgenre = None\n",
    "\n",
    "        if ablation != 'user_info':\n",
    "            user_info = sample['user_info'].to(device)\n",
    "        else:\n",
    "            user_info = None\n",
    "\n",
    "        if ablation != 'title':\n",
    "            mtitle = sample['movie_info_title'].to(device)\n",
    "        else:\n",
    "            mtitle = None\n",
    "\n",
    "        if rnn_type == 'lstm':\n",
    "            lengths = sample['user_history_len_cut'].cpu()\n",
    "        else:\n",
    "            lengths = None\n",
    "\n",
    "        # with torch.cuda.amp.autocast():\n",
    "        #     pred = model(user_history_movie_id, movie_id)\n",
    "\n",
    "        #     loss = criterion(pred, rating.to(torch.float16))\n",
    "\n",
    "        pred = model(item_sequences=user_history_movie_id,\n",
    "                     ratings_sequences=user_history_rating,\n",
    "                     lengths=lengths,\n",
    "                     movies=movie_id,\n",
    "                     user_info=user_info,\n",
    "                     mgenre=mgenre,\n",
    "                     mtitle=mtitle,)\n",
    "        \n",
    "        loss = criterion(pred, rating.to(torch.float32))\n",
    "        loss_running.append(loss.item())\n",
    "\n",
    "        pred_rounded = torch.clamp(pred,1,5).round()\n",
    "        accu = torch.sum(pred_rounded == rating.to(torch.int32)).to(torch.float32) / len(movie_id)\n",
    "        accu_running.append(accu.item())\n",
    "\n",
    "        # we will weight average by batch size later\n",
    "        mean_weights.append(len(movie_id))\n",
    "\n",
    "\n",
    "        # scaler.scale(loss).backward()\n",
    "        # scaler.step(optimizer)\n",
    "        # scaler.update()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            # Change the learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        inner.update(1)\n",
    "        \n",
    "    loss_history.append(np.average(loss_running, weights=mean_weights))\n",
    "    accu_history.append(np.average(accu_running, weights=mean_weights))\n",
    "    \n",
    "    loss_history_running.append(loss_running)\n",
    "    accu_history_running.append(loss_running)\n",
    "\n",
    "\n",
    "\n",
    "    # Evaluate mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():    \n",
    "        curr_loss_val, curr_accu_val = compute_acc(dataloader_val, model)\n",
    "        loss_history_val.append(curr_loss_val)\n",
    "        accu_history_val.append(curr_accu_val)\n",
    "    \n",
    "\n",
    "\n",
    "    # Save the best weights\n",
    "    if curr_loss_val < best_loss_val:\n",
    "        best_loss_val = curr_loss_val\n",
    "        checkpoint = { \n",
    "            'epoch': epoch,\n",
    "            'model': model,\n",
    "            'optimizer': optimizer,\n",
    "            'scheduler': scheduler,\n",
    "            'scaler': scaler}\n",
    "        torch.save(checkpoint, os.path.join(model_folder, f'best_model.pth'))\n",
    "        early_stop_count = 0\n",
    "    \n",
    "    \n",
    "    # Save checkpoint every 10\n",
    "    if epoch % 10 == 0 or epoch >= n_iter-1:\n",
    "        checkpoint = { \n",
    "            'epoch': epoch,\n",
    "            'model': model,\n",
    "            'optimizer': optimizer,\n",
    "            'scheduler': scheduler,\n",
    "            'scaler': scaler}\n",
    "        torch.save(checkpoint, os.path.join(model_folder, f'checkpt{epoch}.pth'))\n",
    "        \n",
    "    # Print the results    \n",
    "    outer.update(1)\n",
    "    print(\"epoch:\", epoch, \\\n",
    "          \"train loss:\", round(loss_history[-1], 6), \\\n",
    "          \"validation loss:\", round(loss_history_val[-1], 6), \\\n",
    "            \"train accuracy:\", round(accu_history[-1], 6), \\\n",
    "          \"validation accuracy:\", round(accu_history_val[-1], 6),\n",
    "         \"next_lr:\", scheduler.get_last_lr())\n",
    "\n",
    "\n",
    "# Save final model \n",
    "checkpoint = { \n",
    "    'epoch': epoch,\n",
    "    'model': model,\n",
    "    'optimizer': optimizer,\n",
    "    'scheduler': scheduler,\n",
    "    'scaler': scaler}\n",
    "torch.save(checkpoint, os.path.join(model_folder, f'final_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print loss history\n",
    "fig, axs = plt.subplots(2, 1, figsize=(7, 7), squeeze=True, sharex=True)\n",
    "\n",
    "# loss_history_running = np.concatenate(loss_history_running)\n",
    "# accu_history_running = np.concatenate(accu_history_running)\n",
    "# lr_history_running = np.concatenate(lr_history_running)\n",
    "\n",
    "# n_epochs = len(loss_history_val)\n",
    "# total_it = len(loss_history_running)\n",
    "\n",
    "x_epoch = range(len(loss_history_val))\n",
    "# x_iter = np.linspace(0,n_epochs,total_it,endpoint=False)\n",
    "\n",
    "axs[0].plot(x_epoch, loss_history, label='training')\n",
    "axs[0].plot(x_epoch, loss_history_val, label='validation')\n",
    "axs[0].set_xlabel('epoch')\n",
    "axs[0].set_ylabel('loss')\n",
    "axs[0].set_title('MSE over Training Epochs')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(x_epoch, accu_history, label='training')\n",
    "axs[1].plot(x_epoch, accu_history_val, label='validation')\n",
    "axs[1].set_xlabel('epoch')\n",
    "axs[1].set_ylabel('Accuracy')\n",
    "axs[1].set_title('Accuracy over Training Epochs')\n",
    "axs[1].legend()\n",
    "\n",
    "# axs[2].plot(x_iter, lr_history_running)\n",
    "# axs[2].set_xlabel('epoch')\n",
    "# axs[2].set_ylabel('Learning Rate')\n",
    "# axs[2].set_title('Learning Rate over Training Epochs')\n",
    "# axs[2].legend()\n",
    "\n",
    "plt.savefig(os.path.join(results_folder, 'LossHistory.png'))\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = torch.load(os.path.join(model_folder, f'best_model.pth'),\n",
    "#                         map_location=torch.device('cpu'))\n",
    "\n",
    "# model = checkpoint['model']\n",
    "# model.to(device)\n",
    "# model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_model(checkpoint, criterion, dataloader_train, dataloader_val, n_iter=5):\n",
    "    \n",
    "    \n",
    "    epoch = checkpoint['epoch']\n",
    "    model = checkpoint['model']\n",
    "    optimizer = checkpoint['optimizer']\n",
    "    scheduler = checkpoint['scheduler']\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=learn_rate/10, epochs=n_iter, steps_per_epoch=len(dataloader_train))\n",
    "    scaler = checkpoint['scaler']\n",
    "    \n",
    "    \n",
    "    best_loss_val = np.inf\n",
    "    # for param_group in optimizer.param_groups:\n",
    "    #     param_group['lr'] = lr\n",
    "    \n",
    "    print(\"Start train...\")\n",
    "    outer = tqdm(total=n_iter, desc='Epochs', position=0)\n",
    "    inner = tqdm(total=len(dataloader_train), desc=f'Batch', position=1)\n",
    "    for epoch in range(n_iter):\n",
    "\n",
    "        #Train mode\n",
    "        model.train()\n",
    "        loss_running = []\n",
    "        accu_running = []\n",
    "        mean_weights = []\n",
    "\n",
    "        inner.refresh()  #force print final state\n",
    "        inner.reset()  #reuse bar    \n",
    "        for _, sample in enumerate(dataloader_train):\n",
    "            optimizer.zero_grad()\n",
    "            if ablation != 'user_history':\n",
    "                user_history_movie_id = sample['user_history_movie_id'].to(device)\n",
    "                if ablation != 'user_ratings':\n",
    "                    user_history_rating = sample['user_history_rating'].to(torch.float32).to(device)\n",
    "                else:\n",
    "                    user_history_rating = None\n",
    "            else:\n",
    "                user_history_movie_id = None\n",
    "                user_history_rating = None\n",
    "\n",
    "            movie_id = sample['movie_id'].to(device)\n",
    "            rating = sample['rating'].to(device)\n",
    "\n",
    "            if ablation != 'genre':\n",
    "                mgenre = sample['movie_info_genres'].to(device)\n",
    "            else:\n",
    "                mgenre = None\n",
    "\n",
    "            if ablation != 'user_info':\n",
    "                user_info = sample['user_info'].to(device)\n",
    "            else:\n",
    "                user_info = None\n",
    "\n",
    "            if ablation != 'title':\n",
    "                mtitle = sample['movie_info_title'].to(device)\n",
    "            else:\n",
    "                mtitle = None\n",
    "\n",
    "            if rnn_type == 'lstm':\n",
    "                lengths = sample['user_history_len_cut'].cpu()\n",
    "            else:\n",
    "                lengths = None\n",
    "\n",
    "            # with torch.cuda.amp.autocast():\n",
    "            #     pred = model(user_history_movie_id, movie_id)\n",
    "\n",
    "            #     loss = criterion(pred, rating.to(torch.float16))\n",
    "\n",
    "            pred = model(item_sequences=user_history_movie_id,\n",
    "                         ratings_sequences=user_history_rating,\n",
    "                         lengths=lengths,\n",
    "                         movies=movie_id,\n",
    "                         user_info=user_info,\n",
    "                         mgenre=mgenre,\n",
    "                         mtitle=mtitle,)\n",
    "\n",
    "            loss = criterion(pred, rating.to(torch.float32))\n",
    "            loss_running.append(loss.item())\n",
    "\n",
    "            pred_rounded = torch.clamp(pred,1,5).round()\n",
    "            accu = torch.sum(pred_rounded == rating.to(torch.int32)).to(torch.float32) / len(movie_id)\n",
    "            accu_running.append(accu.item())\n",
    "\n",
    "            # we will weight average by batch size later\n",
    "            mean_weights.append(len(movie_id))\n",
    "\n",
    "\n",
    "            # scaler.scale(loss).backward()\n",
    "            # scaler.step(optimizer)\n",
    "            # scaler.update()\n",
    "            # print(loss.device, pred.device, rating.device)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "                # Change the learning rate\n",
    "            scheduler.step()\n",
    "\n",
    "            inner.update(1)\n",
    "\n",
    "        loss_history.append(np.average(loss_running, weights=mean_weights))\n",
    "        accu_history.append(np.average(accu_running, weights=mean_weights))\n",
    "\n",
    "\n",
    "\n",
    "        # Evaluate mode\n",
    "        model.eval()\n",
    "        with torch.no_grad():    \n",
    "            curr_loss_val, curr_accu_val = compute_acc(dataloader_val, model)\n",
    "            loss_history_val.append(curr_loss_val)\n",
    "            accu_history_val.append(curr_accu_val)\n",
    "\n",
    "\n",
    "        # Print the results    \n",
    "        outer.update(1)\n",
    "        print(\"epoch:\", epoch, \\\n",
    "              \"train loss:\", round(loss_history[-1], 6), \\\n",
    "              \"validation loss:\", round(loss_history_val[-1], 6), \\\n",
    "                \"train accuracy:\", round(accu_history[-1], 6), \\\n",
    "              \"validation accuracy:\", round(accu_history_val[-1], 6),\n",
    "             \"next_lr:\", scheduler.get_last_lr())\n",
    "        \n",
    "        if curr_loss_val < best_loss_val:\n",
    "            best_loss_val = curr_loss_val\n",
    "            checkpoint = { \n",
    "                'epoch': epoch,\n",
    "                'model': model,\n",
    "                'optimizer': optimizer,\n",
    "                'scheduler': scheduler,\n",
    "                'scaler': scaler}\n",
    "\n",
    "        \n",
    "    return checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ablation_anal(checkpoint, ablation_eval, n_iter=5):\n",
    "    print(ablation_eval)\n",
    "    \n",
    "    \n",
    "    checkpoint['model']\n",
    "    checkpoint['model'].to(device)\n",
    "    checkpoint['model'].eval()\n",
    "    \n",
    "    checkpoint['model'].ablate(ablation_eval)\n",
    "    results = []\n",
    "    results.append(list(compute_acc(dataloader_train, checkpoint['model'])))\n",
    "    results.append(list(compute_acc(dataloader_val, checkpoint['model'])))\n",
    "    results.append(list(compute_acc(dataloader_test, checkpoint['model'])))\n",
    "    \n",
    "    checkpoint = recover_model(checkpoint, criterion, dataloader_train, dataloader_val, n_iter=n_iter)\n",
    "    \n",
    "    checkpoint['model']\n",
    "    checkpoint['model'].to(device)\n",
    "    checkpoint['model'].eval()\n",
    "    \n",
    "    results_recovered = []\n",
    "    results_recovered.append(list(compute_acc(dataloader_train, checkpoint['model'])))\n",
    "    results_recovered.append(list(compute_acc(dataloader_val, checkpoint['model'])))\n",
    "    results_recovered.append(list(compute_acc(dataloader_test, checkpoint['model'])))\n",
    "    \n",
    "    results_df = pd.DataFrame(results, index = ['train','val','test'], columns = ['loss (MSE)', 'accuracy'])\n",
    "    results_recovered_df = pd.DataFrame(results_recovered, index = ['train','val','test'], columns = ['loss (MSE)', 'accuracy'])\n",
    "    \n",
    "    return pd.concat([results_df, results_recovered_df], keys=['ablated', f'recovered {n_iter}'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ablation_evals = ['movie', 'title', 'user_history', 'user_info']\n",
    "ablation_evals = [\n",
    "    (),\n",
    "    ('movie',),\n",
    "    ('title',),\n",
    "    ('user_history',),\n",
    "    ('user_info',),\n",
    "    ('movie', 'title'),\n",
    "    # ('movie', 'user_history'),\n",
    "    # ('movie', 'user_info'),\n",
    "    # ('title', 'user_history'),\n",
    "    # ('title', 'user_info'),\n",
    "    ('user_history', 'user_info'),\n",
    "    # ('movie', 'title', 'user_history'),\n",
    "    # ('movie', 'title', 'user_info'),\n",
    "    # ('movie', 'user_history', 'user_info'),\n",
    "    # ('title', 'user_history', 'user_info'),\n",
    "    ('movie', 'title', 'user_history', 'user_info')\n",
    "    \n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "results_dfs = []\n",
    "for ablation_eval in ablation_evals:\n",
    "    checkpoint = torch.load(os.path.join(model_folder, f'best_model.pth'),\n",
    "                        map_location=torch.device(device))\n",
    "    \n",
    "    results_df = ablation_anal(checkpoint, ablation_eval)\n",
    "    results_dfs.append(results_df)\n",
    "    display(results_df)\n",
    "    \n",
    "full_results_df = pd.concat(results_dfs, keys = map(str, ablation_evals) )\n",
    "full_results_df.index.names = ['ablation', 'set']\n",
    "full_results_df.to_csv(os.path.join(results_folder, 'results.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "model = torch.nn.Sequential(nn.Linear(1,1))\n",
    "\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(\n",
    "                model.parameters(),\n",
    "                weight_decay=l2,\n",
    "                lr=1.25e-2,\n",
    "                betas=(0.9, 0.99),\n",
    "                    eps = 1e-05,\n",
    "            )\n",
    "\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1.25e-2, epochs=20, steps_per_epoch=797, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "for i in range(20):\n",
    "    print(\"epoch: \" + str(i))\n",
    "    for j in range(797):\n",
    "        optimizer.zero_grad()\n",
    "        x = torch.rand(1, 1, requires_grad=True).to(device)\n",
    "        y = torch.rand(1, 1, requires_grad=True).to(device)\n",
    "        y_hat = model(x)\n",
    "        loss = Variable(loss, requires_grad = True)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.lstm.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = torch.load(os.path.join(model_folder, f'best_model.pth'),\n",
    "#                     map_location=torch.device(device))\n",
    "\n",
    "# checkpoint['model']\n",
    "# checkpoint['model'].to(device)\n",
    "# checkpoint['model'].eval()\n",
    "\n",
    "# checkpoint['model'].ablate(('user_history',))\n",
    "\n",
    "# model.lstm.bias_ih_l0.data[model.user_history_dim:2*model.user_history_dim].fill_(-float('inf'))\n",
    "# model.lstm.bias_hh_l0.data[model.user_history_dim:2*model.user_history_dim].fill_(-float('inf'))\n",
    "\n",
    "# results = []\n",
    "# # results.append(list(compute_acc(dataloader_train, checkpoint['model'])))\n",
    "# # results.append(list(compute_acc(dataloader_val, checkpoint['model'])))\n",
    "# results.append(list(compute_acc(dataloader_test, checkpoint['model'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "mLSTMnet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
